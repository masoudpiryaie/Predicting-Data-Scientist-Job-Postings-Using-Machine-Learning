# Creating a comprehensive Jupyter Notebook file for the full pipeline:
from pathlib import Path
import json
notebook = {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Clean, Preprocess & Train Two Models\n",
    "\n",
    "This notebook performs **(1)** cleaning of the unclean dataset `Uncleaned_DS_jobs.csv`, **(2)** preprocessing and feature engineering, and **(3)** trains and evaluates two models implemented in **Keras** and **PyTorch**. The target task used here is a **binary classification**: predicting whether a job posting is for a **Data Scientist** (1) or **Other** (0), derived from `Job Title`.\n",
    "\n",
    "**Important:** Place `Uncleaned_DS_jobs.csv` in the same folder as this notebook before running. If you do not have the file, the cells will check and create a small synthetic sample so you can run the notebook end-to-end as a demo.\n",
    "\n",
    "The notebook outputs:\n",
    "- `Cleaned_DS_jobs.csv` (cleaned dataset)\n",
    "- Model comparison table (accuracy, precision, recall, f1, AUC)\n",
    "- Confusion matrix plots for both models\n",
    "\n",
    "Implemented frameworks: **TensorFlow / Keras** and **PyTorch**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path('Uncleaned_DS_jobs.csv')\n",
    "if not DATA_PATH.exists():\n",
    "    # Create a small synthetic sample so the notebook can run as a demo\n",
    "    demo = pd.DataFrame([\n",
    "        {'Job Title':'Data Scientist','Salary Estimate':'$75K-$131K (Glassdoor est.)','Job Description':'Work on ML models','Rating':'4.0','Company Name':'Acme','Location':'New York, NY','Headquarters':'New York, NY','Size':'51 to 200 employees','Founded':'2010'},\n",
    "        {'Job Title':'Senior Data Engineer','Salary Estimate':'$120K-$160K (Glassdoor est.)','Job Description':'Build data pipelines','Rating':'3.8','Company Name':'BetaCorp','Location':'San Francisco, CA','Headquarters':'San Francisco, CA','Size':'501 to 1000 employees','Founded':'2005'},\n",
    "        {'Job Title':'Machine Learning Engineer','Salary Estimate':np.nan,'Job Description':'Deploy models <br> and monitor','Rating':'-1','Company Name':'Gamma','Location':'Boston, MA','Headquarters':'Boston, MA','Size':'1001 to 5000 employees','Founded':'-1'},\n",
    "        {'Job Title':'Data Scientist','Salary Estimate':'$90K-$150K (Glassdoor est.)','Job Description':'Analysis and modeling','Rating':'4.2','Company Name':'Delta Inc.','Location':'Newark, NJ','Headquarters':'Newark, NJ','Size':'51 to 200 employees','Founded':'2012'},\n",
    "        {'Job Title':None,'Salary Estimate':'$50K-$90K (Glassdoor est.)','Job Description':'Junior role','Rating':'3.5','Company Name':None,'Location':'Remote','Headquarters':'Remote','Size':'Other (433)','Founded':'2018'}\n",
    "    ])\n",
    "    demo.to_csv(DATA_PATH, index=False)\n",
    "    print('Demo dataset created as Uncleaned_DS_jobs.csv')\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Initial shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Cleaning steps\n",
    "We will:\n",
    "- Drop exact duplicates\n",
    "- Clean textual fields (remove HTML tags, trim whitespace)\n",
    "- Convert `Salary Estimate`, `Rating`, `Founded` to numeric fields\n",
    "- Standardize `Size` and extract `City` from `Location`\n",
    "- Create binary target `is_data_scientist` from `Job Title`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df = df.drop_duplicates().copy()\n",
    "\n",
    "def clean_text(x):\n",
    "    if pd.isnull(x):\n",
    "        return ''\n",
    "    x = re.sub(r'<.*?>', '', str(x))\n",
    "    return x.strip()\n",
    "\n",
    "text_cols = ['Job Title','Job Description','Company Name','Location','Headquarters','Size']\n",
    "for c in text_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].apply(clean_text)\n",
    "\n",
    "import numpy as np\n",
    "def clean_salary(s):\n",
    "    if pd.isnull(s) or str(s).strip()=='' or 'nan' in str(s).lower():\n",
    "        return np.nan\n",
    "    s = str(s).replace('(Glassdoor est.)','').replace('$','').replace('K','').replace(',','')\n",
    "    s = s.replace(' - ','-').replace('â€”','-').replace(' to ', '-')\n",
    "    try:\n",
    "        low, high = s.split('-')\n",
    "        return (float(low.strip()) + float(high.strip()))/2\n",
    "    except Exception:\n",
    "        try:\n",
    "            return float(s)\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "if 'Salary Estimate' in df.columns:\n",
    "    df['Salary'] = df['Salary Estimate'].apply(clean_salary)\n",
    "\n",
    "if 'Rating' in df.columns:\n",
    "    df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
    "    # replace negatives or -1 with NaN\n",
    "    df.loc[df['Rating'] < 0, 'Rating'] = np.nan\n",
    "\n",
    "if 'Founded' in df.columns:\n",
    "    df['Founded'] = pd.to_numeric(df['Founded'], errors='coerce')\n",
    "    df.loc[df['Founded'] < 0, 'Founded'] = np.nan\n",
    "\n",
    "# Standardize Size\n",
    "if 'Size' in df.columns:\n",
    "    df['Size'] = df['Size'].replace({'51 to 200 employees':'51-200','1001 to 5000 employees':'1001-5000',\n",
    "                                      '5001 to 10000 employees':'5001-10000','501 to 1000 employees':'501-1000'})\n",
    "\n",
    "# Extract City\n",
    "if 'Location' in df.columns:\n",
    "    df['City'] = df['Location'].apply(lambda x: x.split(',')[0].strip() if ',' in x else x)\n",
    "\n",
    "# Create target: is Data Scientist\n",
    "df['is_data_scientist'] = df['Job Title'].str.contains('data scientist', case=False, na=False).astype(int)\n",
    "\n",
    "# Drop rows without Job Title or Company Name since they are critical\n",
    "df = df.dropna(subset=['Job Title','Company Name'], how='any')\n",
    "\n",
    "# Fill numerical missing values with median (simple approach)\n",
    "for col in ['Salary','Rating','Founded']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "print('After cleaning shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset\n",
    "clean_path = Path('Cleaned_DS_jobs.csv')\n",
    "df.to_csv(clean_path, index=False)\n",
    "print('Saved cleaned dataset to', clean_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Feature engineering\n",
    "We'll build a feature set combining numeric features and a reduced-dimensionality representation of the job description text:\n",
    "- Numeric: Salary, Rating, Founded\n",
    "- Categorical: Size (one-hot), City (top N -> one-hot)\n",
    "- Text: TF-IDF on `Job Description` (max_features=500) then TruncatedSVD to 50 components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare text\n",
    "texts = df['Job Description'].fillna('').astype(str).values\n",
    "tf = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "X_text_tfidf = tf.fit_transform(texts)\n",
    "\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "X_text = svd.fit_transform(X_text_tfidf)\n",
    "\n",
    "# Numeric\n",
    "numeric_cols = [c for c in ['Salary','Rating','Founded'] if c in df.columns]\n",
    "X_num = df[numeric_cols].values\n",
    "\n",
    "# Categorical: Size + top 20 cities\n",
    "cat_cols = []\n",
    "if 'Size' in df.columns:\n",
    "    cat_cols.append('Size')\n",
    "if 'City' in df.columns:\n",
    "    top_cities = df['City'].value_counts().index[:20].tolist()\n",
    "    df['City_top'] = df['City'].apply(lambda x: x if x in top_cities else 'Other')\n",
    "    cat_cols.append('City_top')\n",
    "\n",
    "if cat_cols:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    X_cat = ohe.fit_transform(df[cat_cols])\n",
    "else:\n",
    "    X_cat = np.zeros((df.shape[0],0))\n",
    "\n",
    "# Combine all features\n",
    "X = np.hstack([X_num, X_cat, X_text])\n",
    "y = df['is_data_scientist'].values\n",
    "\n",
    "print('Feature matrix shape:', X.shape)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale numeric part (first len(numeric_cols) columns)\n",
    "scaler = StandardScaler()\n",
    "if len(numeric_cols)>0:\n",
    "    X_train[:, :len(numeric_cols)] = scaler.fit_transform(X_train[:, :len(numeric_cols)])\n",
    "    X_test[:, :len(numeric_cols)] = scaler.transform(X_test[:, :len(numeric_cols)])\n",
    "\n",
    "print('Training samples:', X_train.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Model A: Keras (TensorFlow) - simple MLP\n",
    "Binary classification using a small feed-forward network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(input_dim,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, validation_split=0.15, epochs=50, batch_size=32, callbacks=[es], verbose=1)\n",
    "\n",
    "keras_pred_proba = model.predict(X_test).ravel()\n",
    "keras_pred = (keras_pred_proba >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Model B: PyTorch - simple MLP\n",
    "We'll implement a small feed-forward network and a training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "test_ds = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "model_t = SimpleMLP(X_train.shape[1]).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_t.parameters(), lr=1e-3)\n",
    "\n",
    "def train_epoch(model, loader, opt, crit):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device).unsqueeze(1)\n",
    "        opt.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = crit(out, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ys, yps = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            out = model(xb).cpu().numpy().ravel()\n",
    "            ys.extend(yb.numpy().ravel().tolist())\n",
    "            yps.extend(out.tolist())\n",
    "    return np.array(ys), np.array(yps)\n",
    "\n",
    "best_loss = np.inf\n",
    "for epoch in range(1, 51):\n",
    "    loss = train_epoch(model_t, train_loader, optimizer, criterion)\n",
    "    ys_val, yps_val = evaluate(model_t, test_loader)\n",
    "    val_loss = criterion(torch.tensor(yps_val), torch.tensor(ys_val)).item() if len(ys_val)>0 else 0\n",
    "    print(f'Epoch {epoch}: train_loss={loss:.4f}, val_loss={val_loss:.4f}')\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model_t.state_dict(), 'best_torch_model.pt')\n",
    "\n",
    "model_t.load_state_dict(torch.load('best_torch_model.pt'))\n",
    "y_true_t, y_proba_t = evaluate(model_t, test_loader)\n",
    "torch_pred_proba = y_proba_t\n",
    "torch_pred = (torch_pred_proba >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Evaluation & Comparison\n",
    "Compute accuracy, precision, recall, F1, ROC-AUC and show confusion matrices for both models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def compute_metrics(y_true, y_pred, y_proba):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'auc': roc_auc_score(y_true, y_proba) if len(np.unique(y_true))>1 else np.nan\n",
    "    }\n",
    "\n",
    "keras_metrics = compute_metrics(y_test, keras_pred, keras_pred_proba)\n",
    "torch_metrics = compute_metrics(y_true_t, torch_pred, torch_pred_proba)\n",
    "\n",
    "results = pd.DataFrame([keras_metrics, torch_metrics], index=['Keras_MLP','PyTorch_MLP'])\n",
    "results\n",
    "\n",
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
    "cmk = confusion_matrix(y_test, keras_pred)\n",
    "cmt = confusion_matrix(y_true_t, torch_pred)\n",
    "sns.heatmap(cmk, annot=True, fmt='d', ax=axes[0]); axes[0].set_title('Keras Confusion Matrix'); axes[0].set_xlabel('Pred'); axes[0].set_ylabel('True')\n",
    "sns.heatmap(cmt, annot=True, fmt='d', ax=axes[1]); axes[1].set_title('PyTorch Confusion Matrix'); axes[1].set_xlabel('Pred'); axes[1].set_ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nModel comparison metrics:')\n",
    "display(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Discussion & Next steps\n",
    "- The simple pipeline above demonstrates cleaning, feature engineering, and model training using **Keras** and **PyTorch**.\n",
    "- For improved performance consider:\n",
    "  - More advanced text models (transformers / fine-tuning)\n",
    "  - Better handling of class imbalance (if present) via resampling or weighted loss\n",
    "  - Hyperparameter tuning with cross-validation\n",
    "  - More careful imputation and feature selection\n",
    "\n",
    "### Deliverables\n",
    "- This notebook (HTML export) and `Cleaned_DS_jobs.csv` should be submitted together in Canvas as required by the assignment brief.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Save notebook


out_path = Path("Final_Project_Unclean_to_Models_Full.ipynb")
with open(out_path, "w", encoding="utf-8") as f:
    json.dump(notebook, f, indent=2)

print(f"Notebook created at {out_path.resolve()}")